
<!DOCTYPE html>


<html lang="en" data-content_root="../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>4. Model training &amp; evaluation &#8212; MLOps</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.5.1/css/all.min.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.1/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.1/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.1/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=fa44fd50" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/sphinx-book-theme.css?v=384b581d" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css?v=be8a1c11" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../_static/design-style.1e8bd061cd6da7fc9cf755528e8ffc24.min.css?v=0a3b3ea7" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=8d27b9dea8ad943066ae" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=8d27b9dea8ad943066ae" />
  <script src="../_static/vendor/fontawesome/6.5.1/js/all.min.js?digest=8d27b9dea8ad943066ae"></script>

    <script src="../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../_static/doctools.js?v=9a2dae69"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../_static/copybutton.js?v=f281be69"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?v=efea14e4"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js?v=36754332"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'book/model-training-and-evaluation';</script>
    <link rel="canonical" href="https://aqwqqq.github.io/jupyter-mlops/book/model-training-and-evaluation.html" />
    <link rel="icon" href="../_static/fav.ico"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="5. Model deployment" href="model-deployment.html" />
    <link rel="prev" title="3. Data engineering" href="data-engineering.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a id="pst-skip-link" class="skip-link" href="#main-content">Skip to main content</a>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>
    Back to top
  </button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search..."
         aria-label="Search..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <header class="bd-header navbar navbar-expand-lg bd-navbar">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  

<a class="navbar-brand logo" href="../index.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../_static/logo.png" class="logo__image only-light" alt="MLOps - Home"/>
    <script>document.write(`<img src="../_static/logo.png" class="logo__image only-dark" alt="MLOps - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn navbar-btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Chapters</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="overview.html">1. Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="problem-framing.html">2. Problem framing</a></li>
<li class="toctree-l1"><a class="reference internal" href="data-engineering.html">3. Data engineering</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">4. Model training &amp; evaluation</a></li>
<li class="toctree-l1"><a class="reference internal" href="model-deployment.html">5. Model deployment</a></li>
<li class="toctree-l1"><a class="reference internal" href="markdown.html">6. Markdown Files</a></li>
<li class="toctree-l1"><a class="reference internal" href="notebooks.html">7. Content with notebooks</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Assignments</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="assignments/data-engineering.html">8. Data engineering</a></li>
<li class="toctree-l1"><a class="reference internal" href="assignments/counterintuitive-challenges-in-ml-debugging.html">9. Counterintuitive Challenges in ML Debugging</a></li>
<li class="toctree-l1"><a class="reference internal" href="assignments/debugging-in-classification.html">10. Case Study: Debugging in Classification</a></li>
<li class="toctree-l1"><a class="reference internal" href="assignments/debugging-in-regression.html">11. Case Study: Debugging in Regression</a></li>
<li class="toctree-l1"><a class="reference internal" href="assignments/random-forest-classifier.html">12. Introduction</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-launch-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Launch interactive content">
    <i class="fas fa-rocket"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://mybinder.org/v2/gh/Aqwqqq/jupyter-mlops.git/master?urlpath=tree/book/model-training-and-evaluation.ipynb" target="_blank"
   class="btn btn-sm dropdown-item"
   title="Launch onBinder"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  
    <img src="../_static/images/logo_binder.svg">
  </span>
<span class="btn__text-container">Binder</span>
</a>
</li>
      
      
      
      
      <li><a href="https://colab.research.google.com/github/Aqwqqq/jupyter-mlops.git/blob/master/book/model-training-and-evaluation.ipynb" target="_blank"
   class="btn btn-sm dropdown-item"
   title="Launch onColab"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  
    <img src="../_static/images/logo_colab.png">
  </span>
<span class="btn__text-container">Colab</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/Aqwqqq/jupyter-mlops.git" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/Aqwqqq/jupyter-mlops.git/edit/main/book/model-training-and-evaluation.ipynb" target="_blank"
   class="btn btn-sm btn-source-edit-button dropdown-item"
   title="Suggest edit"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-pencil-alt"></i>
  </span>
<span class="btn__text-container">Suggest edit</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/Aqwqqq/jupyter-mlops.git/issues/new?title=Issue%20on%20page%20%2Fbook/model-training-and-evaluation.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/book/model-training-and-evaluation.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm navbar-btn theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch nav-link" data-mode="light"><i class="fa-solid fa-sun fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="dark"><i class="fa-solid fa-moon fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="auto"><i class="fa-solid fa-circle-half-stroke fa-lg"></i></span>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Model training & evaluation</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#model-selection">4.1. Model selection</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#start-simple">4.1.1. Start simple</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#transfer-learning">4.1.2. Transfer learning</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#automl">4.1.3. AutoML</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#model-evaluation">4.2. Model evaluation</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#evaluate-quality-using-model-metrics">4.2.1. Evaluate quality using model metrics</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#check-metrics-for-important-data-slices">4.2.2. Check metrics for important data slices</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#use-real-world-metrics">4.2.3. Use real-world metrics</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#optimizing-and-satisfying-metrics">4.2.4. Optimizing and satisfying metrics</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#model-debugging-improvement">4.3. Model debugging &amp; improvement</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#data-and-feature-debugging">4.3.1. Data and feature debugging</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#model-debugging">4.3.2. Model debugging</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#interpreting-loss-curves">4.3.3. Interpreting loss curves</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#model-optimization">4.4. Model optimization</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#add-useful-features">4.4.1. Add useful features</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#tune-hyperparameters">4.4.2. Tune hyperparameters</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#tune-model-depth-and-width">4.4.3. Tune model depth and width</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#your-turn">4.5. Your turn! 🚀</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#self-study">4.6. Self study</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#acknowledgments">4.7. Acknowledgments</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Install the necessary dependencies</span>

<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">sys</span>
<span class="o">!{</span>sys.executable<span class="o">}</span><span class="w"> </span>-m<span class="w"> </span>pip<span class="w"> </span>install<span class="w"> </span>--quiet<span class="w"> </span>seaborn<span class="w"> </span>pandas<span class="w"> </span>scikit-learn<span class="w"> </span>numpy<span class="w"> </span>matplotlib<span class="w"> </span>jupyterlab_myst<span class="w"> </span>ipython
</pre></div>
</div>
</div>
</details>
</div>
<section class="tex2jax_ignore mathjax_ignore" id="model-training-evaluation">
<h1><span class="section-number">4. </span>Model training &amp; evaluation<a class="headerlink" href="#model-training-evaluation" title="Link to this heading">#</a></h1>
<p>Machine Learning modeling, including model selection, training, evaluation and debugging, is very important, but only a small component of the entire Machine Learning pipeline. Some might even argue that it’s the easiest component. Details of specific algorithms won’t be discussed in this section. Instead, we will focus on giving an overview picture of how to choose the right model for the problem.</p>
<section id="model-selection">
<h2><span class="section-number">4.1. </span>Model selection<a class="headerlink" href="#model-selection" title="Link to this heading">#</a></h2>
<p>Once problems are framed as one of the common Machine Learning tasks, the typical approaches to solve them can usually be located correspondingly. You should first figure out the category of the problem. Is it supervised or unsupervised? Or regression vs. classification? Does it require generation or only prediction? If it is the former, the models will have to be much harder to learn the latent space of the data.</p>
<p>Note that the model selection also highly depends on how the business problem is defined. For the same problem area, such as a house price prediction task, different targets may result in choosing different Machine Learning models. It can be regression if the required output is raw numbers. But if the goal is to quantize the income into different brackets and predict the bracket, it becomes a classification problem. Similarly, unsupervised learning could be used to learn labels for the data, which could be then used for supervised learning.</p>
<p>Keep in mind that there could be many ways to frame a problem, and the better one could only be known after the chosen models are trained and evaluated. Even though there are hundreds of ways to select and train a Machine Learning model, it is always a good practice to start with simple data, simple feature engineering, and simple model. Besides, transfer learning could be leveraged to reduce the training time in the context of the neural network. Furthermore, AutoML helps to further save time spent from feature engineering to HPO tuning. We will discuss these three useful tricks for selecting models in detail.</p>
<section id="start-simple">
<h3><span class="section-number">4.1.1. </span>Start simple<a class="headerlink" href="#start-simple" title="Link to this heading">#</a></h3>
<p>When searching for a solution, the first goal is to find an effective simple approach for the task. This serves three major purposes.</p>
<ul class="simple">
<li><p>Focus on resolving the confidence about if the model could solve the problem, as additional complexity and potential bugs are avoided.</p></li>
<li><p>Speed up the project iteration, and more complex components could be gradually added and verified step by step.</p></li>
<li><p>At last, it is important to leverage the simplest solution to build up a reasonable baseline for further comparison with the comprehensive model.</p></li>
</ul>
<blockquote>
<div><p>“If you think that machine learning will give you a 100% boost, then a heuristic will get you 50% of the way there.”
– Martin Zinkevich, research scientist at Google</p>
</div></blockquote>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<p>Martin, Z. (n.d.). <a class="reference external" href="https://martin.zinkevich.org/rules_of_ml/rules_of_ml.pdf">Rules of machine learning: Best practices for ml engineering</a>.</p>
</div>
<p>Start simple is about the architecture overall, which does a decent job on the problem. One or more aspects from below could be considered.</p>
<ol class="arabic simple">
<li><p>Simple data – start with the partial dataset if necessary, and easy-to-understand features which are easy to be captured by the model.</p></li>
<li><p>Simple processing - start with no regularization, normalization or other data processing as they may introduce bugs.</p></li>
<li><p>Simple model – start with a less complex model with sensible defaults, prove the feasibility, get a baseline, iterate and improve it gradually.</p></li>
<li><p>Simple problem - simplify the problem itself if possible, or achieve it in multiple steps or through several sub-problems.</p></li>
</ol>
<p>For example, to simply start a house pricing prediction problem, firstly you could consider the most relevant features or a small portion of the data to build a simple linear regression model to get a baseline. Then you could add more features or data to extend the model to a nonlinear model. Or you could try other regressors such as decision trees, ensembles, shallow to deeper neural networks, etc, depending on the type and volume of data.</p>
<p>Overall, there is no need to pursue the state-of-the-art approach or the most optimized performance at the very beginning. But it is necessary to keep the eyes on the trap of increasingly complex heuristics.</p>
</section>
<section id="transfer-learning">
<h3><span class="section-number">4.1.2. </span>Transfer learning<a class="headerlink" href="#transfer-learning" title="Link to this heading">#</a></h3>
<p><a class="reference external" href="https://en.wikipedia.org/wiki/Transfer_learning">Transfer learning</a> is a research problem in Machine Learning that focuses on storing knowledge gained while solving one problem and applying it to a different but related problem.</p>
<p>Using a large amount of data and tackling a completely new Machine Learning problem can be very challenging sometimes. Transfer learning could be the starting point if the simplified solution does not work or perform well. It allows utilizing knowledge(model) acquired (trained) for one task to solve other similar tasks.</p>
<p>What’s more, transfer learning is very commonly adopted in hot areas such as computer vision and natural language processing. It usually gives significantly better performance than training a simple model. And it is even a rare case to train a model from scratch in such areas. Instead, researchers and data scientists prefer starting from a pre-trained model that already learned general features and how to classify objects.</p>
<figure class="align-default" id="id1">
<img alt="https://static-1300131294.cos.ap-shanghai.myqcloud.com/images/transfer-learning.jpeg" src="https://static-1300131294.cos.ap-shanghai.myqcloud.com/images/transfer-learning.jpeg" />
<figcaption>
<p><span class="caption-number">Fig. 4.1 </span><span class="caption-text"><a class="reference external" href="https://www.geeksforgeeks.org/ml-introduction-to-transfer-learning">Transfer learning</a></span><a class="headerlink" href="#id1" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p>Traditionally, there are three major categories of transfer learning strategies and techniques based on the characteristics of the problem and the data. <strong>Inductive transfer learning</strong> is used if the domains are the same between the source and target, but the exact tasks are not. If the problems’ domains are even different, <strong>transductive transfer learning</strong> could be the choice. <strong>Unsupervised transfer learning</strong> is similar to inductive transfer learning, but for unsupervised tasks with unlabeled datasets both in the source and target.</p>
<figure class="align-default" id="transfer-learning-strategies">
<img alt="https://static-1300131294.cos.ap-shanghai.myqcloud.com/images/transfer-learning-strategies.png" src="https://static-1300131294.cos.ap-shanghai.myqcloud.com/images/transfer-learning-strategies.png" />
<figcaption>
<p><span class="caption-number">Fig. 4.2 </span><span class="caption-text"><a class="reference external" href="https://www.v7labs.com/blog/transfer-learning-guide,%20https://www.v7labs.com/blog/transfer-learning-guide">Transfer learning strategies</a></span><a class="headerlink" href="#transfer-learning-strategies" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p>Under the deep learning context, there are multiple levels of complexity in using a pre-trained model. The most straightforward way is to use the pre-trained model directly, but may not be applicable mostly. An idea here is to leverage the pre-trained model’s weighted layers to extract features while retraining the last layer. One step further, a more engaging technique is to fine-tune and train all layers after starting with only the feature layers. If it still does not work, fully training all the layers will be the fallback.</p>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<p><a class="reference external" href="https://www.v7labs.com/blog/transfer-learning-guide">What is transfer learning? [Examples &amp; newbie-friendly guide]</a>. (n.d.). Retrieved 27 July 2022.</p>
</div>
</section>
<section id="automl">
<h3><span class="section-number">4.1.3. </span>AutoML<a class="headerlink" href="#automl" title="Link to this heading">#</a></h3>
<p>Automated Machine Learning(AutoML) provides methods and processes to make Machine Learning available for non-Machine Learning experts, to improve the efficiency of Machine Learning and accelerate research on Machine Learning. Designing and tuning Machine Learning systems is a labor and time-intensive task, and also requires extensive expertise. AutoML is focused on automating the model selection and training process.</p>
<p>As it is named, AutoML helps automate many aspects of Machine Learning model developments and training. It consists of a broader group of methodologies listed here:</p>
<ul class="simple">
<li><p>Automated Data Clean (Auto Clean)</p></li>
<li><p>Automated Feature Engineering (Auto FE)</p></li>
<li><p>Hyperparameter Optimization (HPO)</p></li>
<li><p>Meta-Learning</p></li>
<li><p>Neural Architecture Search (NAS)</p></li>
</ul>
<p>Today, there are plenty of AutoML tools existing. It is important to understand the strengths and weaknesses of each other before going deep with any of them. <a class="reference external" href="https://openml.github.io/automlbenchmark/index.html">AMLB</a> provides an open and extensible benchmark to help compare and choose the right AutoML frameworks.</p>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<p>Hutter, Frank, Lars Kotthoff, and Joaquin Vanschoren. <a class="reference external" href="https://www.automl.org/book/">Automated machine learning: methods, systems, challenges</a>). Springer Nature, 2019.</p>
</div>
</section>
</section>
<section id="model-evaluation">
<h2><span class="section-number">4.2. </span>Model evaluation<a class="headerlink" href="#model-evaluation" title="Link to this heading">#</a></h2>
<p>In practice, it is challenging to detect if the model is properly learning without under-fitting or overfitting. Ideally, before moving to production, it is always necessary to evaluate the trained model to make sure that everything is working properly. A common approach is to divide the dataset into three parts - training set, validation set and test set.</p>
<p>The model is trained by using only the train set, and the validation set is used to track the progress and conclude to optimize the model. Then the test set is used to evaluate the performance of the model. Using completely new data allows us to get an unbiased opinion on how well the algorithm works.</p>
<figure class="align-default" id="recommended-method-of-deviding-the-dataset">
<img alt="https://static-1300131294.cos.ap-shanghai.myqcloud.com/images/recommended-method-of-deviding-the-dataset.png" src="https://static-1300131294.cos.ap-shanghai.myqcloud.com/images/recommended-method-of-deviding-the-dataset.png" />
<figcaption>
<p><span class="caption-number">Fig. 4.3 </span><span class="caption-text"><a class="reference external" href="https://towardsdatascience.com/preventing-deep-neural-network-from-overfitting-953458db800a">Recommended method of dividing the data set</a></span><a class="headerlink" href="#recommended-method-of-deviding-the-dataset" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p>There is no strict heuristic about how to split the dataset, especially when working with big data. The split ratios depend greatly on the specific problem and data volume. Generally speaking, the train vs. validation vs. test split should allow for:</p>
<ul class="simple">
<li><p>Large enough validation set to compare the difference between models,</p></li>
<li><p>Large enough test set to be representative of overall performance.</p></li>
</ul>
<p>However, while evaluating a Machine Learning model can seem daunting, model metrics show where to start. The following sections discuss how to evaluate performance using metrics.</p>
<section id="evaluate-quality-using-model-metrics">
<h3><span class="section-number">4.2.1. </span>Evaluate quality using model metrics<a class="headerlink" href="#evaluate-quality-using-model-metrics" title="Link to this heading">#</a></h3>
<p>To evaluate your model’s quality, commonly-used metrics are:</p>
<ul class="simple">
<li><p><a class="reference external" href="https://developers.google.com/machine-learning/crash-course/descending-into-ml/training-and-loss">loss</a></p></li>
<li><p><a class="reference external" href="https://developers.google.com/machine-learning/crash-course/classification/accuracy">accuracy</a></p></li>
<li><p><a class="reference external" href="https://developers.google.com/machine-learning/crash-course/classification/precision-and-recall">precision &amp; recall</a></p></li>
<li><p><a class="reference external" href="https://developers.google.com/machine-learning/crash-course/classification/roc-and-auc">area under the ROC curve (AUC)</a></p></li>
</ul>
<p>For guidance on interpreting these metrics, read the linked content from Machine Learning Crash Content. For additional guidance on specific problems, see the following table.</p>
<table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Problem</p></th>
<th class="head"><p>Evaluating Quality</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Regression</p></td>
<td><p>Besides reducing the absolute <a class="reference external" href="https://developers.google.com/machine-learning/crash-course/descending-into-ml/training-and-loss">Mean Square Error</a> (MSE), reduce the MSE relative to the label values. For example, assume to predict prices of two items that have mean prices of 5 and 100. In both cases, assume the MSE is 5. In the first case, the MSE is 100% of your mean price, which is clearly a large error. In the second case, the MSE is 5% of the mean price, which is a reasonable error.</p></td>
</tr>
<tr class="row-odd"><td><p>Multiclass classification</p></td>
<td><p>To predict a small number of classes, look at per-class metrics individually. When predicting on many classes, the per-class metrics can be leveraged to track overall classification metrics. Alternatively, specific quality goals can be prioritized depending on the needs. For example, if to classify objects in images, then the classification quality may be prioritized for people over other objects.</p></td>
</tr>
<tr class="row-even"><td><p>Ranking metrics</p></td>
<td><p>MRR, MAR, ordered logit</p></td>
</tr>
<tr class="row-odd"><td><p>Computer vision</p></td>
<td><p>IoU, Pixel Accuracy</p></td>
</tr>
<tr class="row-even"><td><p>NLP</p></td>
<td><p>Perplexity, BLEU, ROUGE</p></td>
</tr>
</tbody>
</table>
<p>Ideally, choose one single metric to optimize at once; if the requirement is to include multiple metrics, then consider a unified metric.</p>
<p>However, since the real world does not always go as what is imagined, it may be the case to try many metrics before finding one that could be satisfied with, and the metrics may change alongside the development, or even after getting the model into production.</p>
</section>
<section id="check-metrics-for-important-data-slices">
<h3><span class="section-number">4.2.2. </span>Check metrics for important data slices<a class="headerlink" href="#check-metrics-for-important-data-slices" title="Link to this heading">#</a></h3>
<p>After having a high-quality model, the model might still perform poorly on subsets of the data. For example, the unicorn predictor must predict well both in the Sahara desert and in New York City, and at all times of the day. However, there is less training data for the Sahara desert. Therefore, it is necessary to track model quality specifically for the Sahara desert. Such subsets of data, like the subset corresponding to the Sahara desert, are called <strong>data slices</strong>. Data slices should be separately monitored where performance is especially important or where the model might perform poorly.</p>
<p>Use the understanding of the data to identify data slices of interest. Then compare model metrics for data slices against the metrics for the entire data set. Checking that the model performs across all data slices helps remove bias. For more, see <a class="reference external" href="https://developers.google.com/machine-learning/crash-course/fairness/evaluating-for-bias">Fairness: Evaluating for Bias</a>.</p>
</section>
<section id="use-real-world-metrics">
<h3><span class="section-number">4.2.3. </span>Use real-world metrics<a class="headerlink" href="#use-real-world-metrics" title="Link to this heading">#</a></h3>
<p>Model metrics do not necessarily measure the real-world impact of the model. For example, the AUC could be increased by changing a hyperparameter, but how did the change affect the user experience? To measure real-world impact, separate metrics need to be defined. Measuring real-world impact helps compare the quality of different iterations of the model.</p>
</section>
<section id="optimizing-and-satisfying-metrics">
<h3><span class="section-number">4.2.4. </span>Optimizing and satisfying metrics<a class="headerlink" href="#optimizing-and-satisfying-metrics" title="Link to this heading">#</a></h3>
<p>As models are getting bigger and more resource-intensive, how to scale the mode training becomes more and more important. Thinking beyond the above model metrics, the model utilitarian performance should also be considered. It includes the training speed, inference speed, model size, model stability, etc.</p>
<p>To the given example <a class="reference external" href="https://github.com/ajaymache/machine-learning-yearning">ng2017mlyearning</a> below, both model accuracy and running time are important to decide which is the best classifier. It may not be natural to derive a single metric from them. Instead, a more real-world thinking-based strategy could be applied. The running time is important, but mostly it could be acceptable once under a certain value, such as 100ms. Whenever this condition is satisfied and the running time is good enough for production, then accuracy needs to be optimized with the best effort. Here, the running time is the satisfying metric and the accuracy is the optimizing metric.</p>
<figure class="align-default">
<img alt="https://static-1300131294.cos.ap-shanghai.myqcloud.com/images/optimizing-and-satisficing-metrics.png" src="https://static-1300131294.cos.ap-shanghai.myqcloud.com/images/optimizing-and-satisficing-metrics.png" />
</figure>
<p>Optimizing and satisfying metrics could also be applied to evaluate the model among different model metrics. As a final example, to build a natural language processing powered smart speaker’s device like Alexa, the wake word detection module is the key to support using a microphone to listen for the user saying a particular “wake-word” to wake up the system, such as the “Alexa” for Amazon Echo.</p>
<p>The false positive rate is one of the key metrics, which is about the frequency of the system waking up even when no one says the wake-word. While the false negative rate describes how often it fails to wake up when someone says the wake-word. It is difficult to optimize both of them at the same time. Instead, one reasonable way is to have the false negative rate as the optimizing metric which needs to be minimized. And the false positive could be treated as the satisfying metric, which should happen no more than once every 24 hours of operation.</p>
</section>
</section>
<section id="model-debugging-improvement">
<h2><span class="section-number">4.3. </span>Model debugging &amp; improvement<a class="headerlink" href="#model-debugging-improvement" title="Link to this heading">#</a></h2>
<p>Once the model is working, the next step is to optimize the model’s quality for production readiness. Both debugging and optimizing are critical in the Machine Learning pipeline.</p>
<p><strong>How is Machine Learning debugging different?</strong> Before diving into any particular Machine Learning debugging method, it is important to understand what differentiates debugging Machine Learning models from traditional software programs. Unlike the latter, an Machine Learning model with poor quality usually does not imply the presence of a bug. Instead, There could be many reasons to cause a model not to perform well. So that to debug poor performance in a model, a broader range of potential causes need to be investigated compared to traditional programming.</p>
<p>For example, here are a few causes for poor model performance:</p>
<ul class="simple">
<li><p>Theoretical constraints, such as wrong assumptions, unsuccessful problem framing, and poor model/data fit.</p></li>
<li><p>Data contains errors and anomalies or is over-preprocessed.</p></li>
<li><p>Features lack predictive power.</p></li>
<li><p>Poor feature engineering code contains bugs.</p></li>
<li><p>Poor model implementation.</p></li>
<li><p>Hyperparameters are set to non-optimal values.</p></li>
</ul>
<p>Debugging Machine Learning models is complicated by the time it takes to run your experiments. Given the longer iteration cycles, and the larger error space, debugging Machine Learning models is uniquely challenging.</p>
<section id="data-and-feature-debugging">
<h3><span class="section-number">4.3.1. </span>Data and feature debugging<a class="headerlink" href="#data-and-feature-debugging" title="Link to this heading">#</a></h3>
<p>Low-quality data will significantly affect your model’s performance. It’s much easier to detect low-quality data at input instead of guessing at its existence after the model predicts badly. Monitor the data by following the advice in this section.</p>
<p><strong>Validate input data using rules.</strong></p>
<p>To monitor the data, one approach is to write rules that the data must satisfy, and continuously check the data against the expected <a class="reference internal" href="#data-quality"><span class="xref myst">data quality</span></a>. This collection of rules is defined by following these steps:</p>
<ol class="arabic simple">
<li><p>For the feature data, understand the range and distribution. For categorical features, understand the set of possible values.</p></li>
<li><p>Encode the understanding into rules. Examples of rules are:</p>
<ol class="arabic simple">
<li><p>Ensure that user-submitted ratings are always between 1 and 5.</p></li>
<li><p>Check that “the” occurs most frequently (for an English text feature).</p></li>
<li><p>Check that categorical features have values from a fixed set.</p></li>
</ol>
</li>
<li><p>Test the data against the rules which should catch data errors such as:</p>
<ol class="arabic simple">
<li><p>anomalies.</p></li>
<li><p>unexpected values of categorical variables.</p></li>
<li><p>unexpected data distributions.</p></li>
</ol>
</li>
</ol>
<p><strong>Ensure splits are good quality.</strong></p>
<p>The test and training splits must be equally representative of the input data. If the test and training splits are statistically different, then training data will not help predict the test data. It’s often a struggle to gather enough data for a machine learning project. Sometimes, however, there is too much data, and a subset of examples must be selected for training.</p>
<p>Monitor the statistical properties of the splits. If the properties diverge, raise a flag. Further, test that the ratio of examples in each split stays constant. For example, if the data is split 80:20, that ratio should not change.</p>
<p><strong>Test processed data.</strong></p>
<p>While the raw data might be valid, the model only sees processed feature data. Because processed data looks very different from raw input data, it is necessary to check processed data separately. Based on the understanding of the processed data, write unit tests to verify if the data quality assurance is successfully applied through the data engineering process. For example, unit tests could check the following conditions:</p>
<ol class="arabic simple">
<li><p>All numeric features are scaled, for example, between 0 and 1.</p></li>
<li><p>One-hot encoded vectors only contain a single 1 and N-1 zeroes.</p></li>
<li><p>Missing data is replaced by mean or default values.</p></li>
<li><p>Data distributions after transformation conform to expectations. For example, if the data is normalized by using z-scores, the mean of the z-scores is 0.</p></li>
<li><p>Outliers are handled, such as by scaling or clipping.</p></li>
</ol>
</section>
<section id="model-debugging">
<h3><span class="section-number">4.3.2. </span>Model debugging<a class="headerlink" href="#model-debugging" title="Link to this heading">#</a></h3>
<p>After debugging the data, follow these steps to continue debugging the model.</p>
<p><strong>Check that the model can predict labels.</strong></p>
<p>Before debugging the model, try to determine whether the features encode predictive signals. Linear correlations could be found between individual features and labels by using correlation matrices.</p>
<p>However, correlation matrices will not detect nonlinear correlations between features and labels. Instead, choose 10 examples from the dataset that the model can easily learn from. Alternatively, use synthetic data that is easily learnable. For instance, a classifier can easily learn linearly-separable examples while a regressor can easily learn labels that correlate highly with a <a class="reference external" href="https://developers.google.com/machine-learning/glossary/#feature_cross">feature cross</a>. Then, ensure the model can achieve a very small loss on these 10 easily-learnable examples.</p>
<p>Then using a few examples that are easily learnable simplifies debugging by reducing the opportunities for bugs. If it does not work well, consider to further simplifying the model by switching to the simpler gradient descent algorithm instead of a more advanced optimization algorithm.</p>
<p><strong>Establish a baseline.</strong></p>
<p>Comparing the model against a baseline is a quick test of the model’s quality. When developing a new model, define a baseline by using <a class="reference internal" href="#start-simple"><span class="xref myst">a simple heuristic</span></a> to predict the label. If the trained model performs worse than its baseline, it needs to be improved.</p>
<p>Examples of baselines are:</p>
<ul class="simple">
<li><p>Using a linear model trained solely on the most predictive feature.</p></li>
<li><p>In classification, always predict the most common label.</p></li>
<li><p>In regression, always predicting the mean value.</p></li>
</ul>
<p>Once a version of the model is validated in production, it could be used as a baseline for newer model versions. Therefore, there could be multiple baselines of different complexities. Testing against baselines helps justify adding complexity to the model. A more complex model should always perform better than a less complex model or baseline.</p>
<p><strong>Implement tests for Machine Learning code.</strong></p>
<p>The testing process to catch bugs in Machine Learning code is similar to the testing process in traditional debugging. Unit tests could be added to detect bugs. Examples of code bugs in Machine Learning are:</p>
<ul class="simple">
<li><p>Hidden layers that are configured incorrectly.</p></li>
<li><p>Data normalization code that returns NaNs.</p></li>
</ul>
<p>A sanity check for the presence of code bugs is to include the label in the features and train the model. If the model does not work, then it has a bug.</p>
<p><strong>Adjust hyperparameter values.</strong></p>
<p>The table below explains how to adjust values for the hyperparameters.</p>
<table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Hyperparameter</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Learning Rate</p></td>
<td><p>Typically, ML libraries will automatically set the learning rate. For example, in TensorFlow, most TF Estimators use the <a class="reference external" href="https://www.tensorflow.org/api_docs/python/tf/train/AdagradOptimizer">AdagradOptimizer</a>, which sets the learning rate at 0.05 and then adaptively modifies the learning rate during training. The other popular optimizer, <a class="reference external" href="https://www.tensorflow.org/api_docs/python/tf/train/AdamOptimizer">AdamOptimizer</a>, uses an initial learning rate of 0.001. However, if your model does not converge with the default values, then manually choose a value between 0.0001 and 1.0, and increase or decrease the value on a logarithmic scale until your model converges. Remember that the more difficult your problem, the more epochs your model must train for before loss starts to decrease.</p></td>
</tr>
<tr class="row-odd"><td><p>Regularization</p></td>
<td><p>First, ensure your model can predict without regularization on the training data. Then add regularization only if your model is overfitting on training data. Regularization methods differ for linear and nonlinear models.<br><br>For linear models, choose L1 regularization if you need to reduce your model’s size. Choose L2 regularization if you prefer increased model stability. Increasing your model’s stability makes your model training more reproducible. Find the correct value of the regularization rate, , by starting at 1e-5 and tuning that value through trial and error.<br><br>To regularize a deep neural network model, use <a class="reference external" href="https://developers.google.com/machine-learning/glossary/#dropout_regularization">Dropout regularization</a>. Dropout removes a random selection of a fixed percentage of the neurons in a network layer for a single gradient step. Typically, dropout will improve generalization at a dropout rate of between 10% and 50% of neurons.</p></td>
</tr>
<tr class="row-even"><td><p>Training epochs</p></td>
<td><p>You should train for at least one epoch, and continue to train so long as you are not overfitting.</p></td>
</tr>
<tr class="row-odd"><td><p>Batch size</p></td>
<td><p>Typically, the batch size of a <a class="reference external" href="https://developers.google.com/machine-learning/glossary/#mini-batch">mini-batch</a> is between 10 and 1000. For <a class="reference external" href="https://developers.google.com/machine-learning/glossary/#SGD">SGD</a>, the batch size is 1. The upper bound on your batch size is limited by the amount of data that can fit in your machine’s memory. The lower bound on batch size depends on your data and algorithm. However, using a smaller batch size lets your gradient update more often per epoch, which can result in a larger decrease in loss per epoch. Furthermore, models trained using smaller batches generalize better. For details, see <a class="reference external" href="https://arxiv.org/pdf/1609.04836.pdf">On large-batch training for deep learning: Generalization gap and sharp minima</a> N. S. Keskar, D. Mudigere, J. Nocedal, M. Smelyanskiy, and P. T. P. Tang. ICLR, 2017. Prefer using the smallest batch sizes that result in stable training.</p></td>
</tr>
<tr class="row-even"><td><p>Depth and width of layers</p></td>
<td><p>In a neural network, depth refers to the number of layers, and width refers to the number of neurons per layer. Increase depth and width as the complexity of the corresponding problem increases. Adjust your depth and width by following these steps:<br><br>1. Start with 1 fully-connected hidden layer with the same width as your input layer.<br>2. For regression, set the output layer’s width to 1. For classification, set the output layer’s width to the number of classes.<br>3. If your model does not work, and you think your model needs to be deeper to learn your problem, then increase depth linearly by adding a fully-connected hidden layer at a time. The hidden layer’s width depends on your problem. A commonly-used approach is to use the same width as the previous hidden layer, and then discover the appropriate width through trial-and-error.<br><br>The change in width of successive layers also depends on your problem. A practice drawn from common observation is to set a layer’s width equal to or less than the width of the previous layer. Remember, the depth and width don’t have to be exactly right. You’ll tune their values later when you optimize your model.</p></td>
</tr>
</tbody>
</table>
</section>
<section id="interpreting-loss-curves">
<h3><span class="section-number">4.3.3. </span>Interpreting loss curves<a class="headerlink" href="#interpreting-loss-curves" title="Link to this heading">#</a></h3>
<p>Machine learning would be a breeze if all our <a class="reference external" href="https://developers.google.com/machine-learning/crash-course/descending-into-ml/training-and-loss">loss curves</a> looked like this the first time we trained our model:</p>
<figure class="align-default">
<img alt="https://static-1300131294.cos.ap-shanghai.myqcloud.com/images/metric-curve-ideal.svg" src="https://static-1300131294.cos.ap-shanghai.myqcloud.com/images/metric-curve-ideal.svg" /></figure>
<p>But in reality, loss curves can be quite challenging to interpret. Use your understanding of loss curves to answer the following questions.</p>
<p><strong>1. My model won’t train!</strong></p>
<p>Your friend Mel and you continue working on a unicorn appearance predictor. Here’s your first loss curve.</p>
<figure class="align-default">
<img alt="https://static-1300131294.cos.ap-shanghai.myqcloud.com/images/metric-curve-ex03.svg" src="https://static-1300131294.cos.ap-shanghai.myqcloud.com/images/metric-curve-ex03.svg" /></figure>
<p>Your model is not converging. Try these debugging steps:</p>
<ul class="simple">
<li><p>Check if your features can predict the labels by following the steps in <a class="reference internal" href="#model-debugging"><span class="xref myst">Model debugging</span></a>.</p></li>
<li><p>Check your data against a rules to detect bad examples.</p></li>
<li><p>If training looks unstable, as in this plot, then reduce your learning rate to prevent the model from bouncing around in parameter space.</p></li>
<li><p>Simplify your dataset to 10 examples that you know your model can predict. Obtain a very low loss on the reduced dataset. Then continue debugging your model on the full dataset.</p></li>
<li><p>Simplify your model and ensure the model outperforms your baseline. Then incrementally add complexity to the model.</p></li>
</ul>
<p><strong>2. My loss exploded!</strong></p>
<p>Mel shows you another curve. What’s going wrong here and how can she fix it? Write your answer below.</p>
<figure class="align-default">
<img alt="https://static-1300131294.cos.ap-shanghai.myqcloud.com/images/metric-curve-ex02.svg" src="https://static-1300131294.cos.ap-shanghai.myqcloud.com/images/metric-curve-ex02.svg" /></figure>
<p>A large increase in loss is typically caused by anomalous values in input data. Possible causes are:</p>
<ul class="simple">
<li><p>NaNs in input data.</p></li>
<li><p>Exploding gradient due to anomalous data.</p></li>
<li><p>Division by zero.</p></li>
<li><p>Logarithm of zero or negative numbers.</p></li>
</ul>
<p>To fix an exploding loss, check for anomalous data in your batches, and in your engineered data. If the anomaly appears problematic, then investigate the cause. Otherwise, if the anomaly looks like outlying data, then ensure the outliers are evenly distributed between batches by shuffling your data.</p>
<p><strong>3. My metrics are contradictory!</strong></p>
<p>Mel wants your take on another curve. What’s going wrong and how can she fix it? Write your answer below.</p>
<figure class="align-default">
<img alt="https://static-1300131294.cos.ap-shanghai.myqcloud.com/images/metric-curve-ex04.svg" src="https://static-1300131294.cos.ap-shanghai.myqcloud.com/images/metric-curve-ex04.svg" /></figure>
<p>The recall is stuck at 0 because your examples’ classification probability is never higher than the <a class="reference external" href="https://developers.google.com/machine-learning/glossary#classification_threshold">threshold</a> for positive classification. This situation often occurs in problems with a large <a class="reference external" href="https://developers.google.com/machine-learning/glossary#class_imbalanced_data_set">class imbalance</a>. Remember that ML libraries, such as TF Keras, typically use a default threshold of 0.5 to calculate classification metrics.</p>
<p>Try these steps:</p>
<ul class="simple">
<li><p>Lower your classification threshold.</p></li>
<li><p>Check threshold-invariant metrics, such as AUC.</p></li>
</ul>
<p><strong>4. Testing loss is too damn high!</strong></p>
<p>Mel shows you the loss curves for training and testing datasets and asks “What’s wrong?” Write your answer below.</p>
<figure class="align-default">
<img alt="https://static-1300131294.cos.ap-shanghai.myqcloud.com/images/metric-curve-ex01.svg" src="https://static-1300131294.cos.ap-shanghai.myqcloud.com/images/metric-curve-ex01.svg" /></figure>
<p>Your model is overfitting to the training data. Try these steps:</p>
<ul class="simple">
<li><p>Reduce model capacity.</p></li>
<li><p>Add regularization.</p></li>
<li><p>Check that the training and test splits are statistically equivalent.</p></li>
</ul>
<p><strong>5. My model gets stuck.</strong></p>
<p>You’re patient when Mel returns a few days later with yet another curve. What’s going wrong here and how can Mel fix it?</p>
<figure class="align-default">
<img alt="https://static-1300131294.cos.ap-shanghai.myqcloud.com/images/metric-curve-ex05.svg" src="https://static-1300131294.cos.ap-shanghai.myqcloud.com/images/metric-curve-ex05.svg" /></figure>
<p>Your loss is showing repetitive, step-like behavior. The input data seen by your model probably is itself exhibiting repetitive behavior. Ensure that shuffling is removing repetitive behavior from input data.</p>
<p><strong>It’s working!</strong></p>
<p>“It’s working perfectly now!” Mel exclaims. She leans back into her chair triumphantly and heaves a big sigh. The curve looks great and you beam with accomplishment. Mel and you take a moment to discuss the following additional checks for validating your model.</p>
<ul class="simple">
<li><p>real-world metrics</p></li>
<li><p>baselines</p></li>
<li><p>absolute loss for regression problems</p></li>
<li><p>other metrics for classification problems</p></li>
</ul>
<figure class="align-default">
<img alt="https://static-1300131294.cos.ap-shanghai.myqcloud.com/images/metric-curve-ex06.svg" src="https://static-1300131294.cos.ap-shanghai.myqcloud.com/images/metric-curve-ex06.svg" /></figure>
</section>
</section>
<section id="model-optimization">
<h2><span class="section-number">4.4. </span>Model optimization<a class="headerlink" href="#model-optimization" title="Link to this heading">#</a></h2>
<p>Once the model is working, it’s time to optimize the model’s quality. Follow the steps below.</p>
<section id="add-useful-features">
<h3><span class="section-number">4.4.1. </span>Add useful features<a class="headerlink" href="#add-useful-features" title="Link to this heading">#</a></h3>
<p>The model performance could be improved by adding features that encode information not yet encoded by the existing features. Correlation matrices could be used to find linear correlations between individual features and labels. To detect nonlinear correlations between features and labels, the model must be trained with and without the feature, or combination of features, and check for an increase in model quality. The feature’s inclusion must be justified by an increase in model quality.</p>
</section>
<section id="tune-hyperparameters">
<h3><span class="section-number">4.4.2. </span>Tune hyperparameters<a class="headerlink" href="#tune-hyperparameters" title="Link to this heading">#</a></h3>
<p>Values of hyperparameters make your model work. However, these hyperparameter values can still be tuned. The values could be tuned manually by trial and error, but manual tuning is time-consuming. Instead, consider using an AutoML hyperparameter tuning service, such as <a class="reference external" href="https://cloud.google.com/ml-engine/docs/tensorflow/hyperparameter-tuning-overview">Google Cloud Machine Learning hyperparameter tuning</a>, <a class="reference external" href="https://github.com/awslabs/autogluon">Auto Gluon</a>, etc. With different sets of hyperparameters, the same model may perform drastically differently on the same dataset. Keep in mind that not all hyperparameters are created equal. A model could be more sensitive to one hyperparameter.</p>
</section>
<section id="tune-model-depth-and-width">
<h3><span class="section-number">4.4.3. </span>Tune model depth and width<a class="headerlink" href="#tune-model-depth-and-width" title="Link to this heading">#</a></h3>
<p>While debugging the model, its depth and width are increased to improve the model performance In contrast, during model optimization, the mode depth and width could be either increased or decreased depending on the goals. If the model quality is adequate, then try reducing overfitting and training time by decreasing depth and width. Specifically, try halving the width at each successive layer. Since the model quality will also decrease, it is always a trade-off to balance quality with overfitting and training time.</p>
<p>Conversely, if the goal is to have higher model quality, then try increasing depth and width. Remember that increases in depth and width are practically limited by accompanying increases in training time and overfitting. To understand overfitting.</p>
<p>Since depth and width are hyperparameters, hyperparameter tuning could be used to optimize depth and width.</p>
</section>
</section>
<section id="your-turn">
<h2><span class="section-number">4.5. </span>Your turn! 🚀<a class="headerlink" href="#your-turn" title="Link to this heading">#</a></h2>
<p>Understanding the challenges in Machine Learning debugging by completing the <a class="reference internal" href="#../assignments/machine-learning-productionization/counterintuitive-challenges-in-ml-debugging.ipynb"><span class="xref myst">Counterintuitive Challenges in ML Debugging</span></a>.</p>
<p>Apply the debugging concepts learned by completing the following:</p>
<ul class="simple">
<li><p><a class="reference internal" href="#../assignments/machine-learning-productionization/debugging-in-regression.ipynb"><span class="xref myst">Case Study: Debugging in Regression</span></a></p></li>
<li><p><a class="reference internal" href="#../assignments/machine-learning-productionization/debugging-in-classification.ipynb"><span class="xref myst">Case Study: Debugging in Classification</span></a></p></li>
</ul>
</section>
<section id="self-study">
<h2><span class="section-number">4.6. </span>Self study<a class="headerlink" href="#self-study" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p><a class="reference external" href="https://blog.statsbot.co/machine-learning-algorithms-183cc73197c">Machine Learning Algorithms: Which One to Choose for Your Problem</a> by Daniil Korbut, Stats and Bots, 2017.</p></li>
</ul>
</section>
<section id="acknowledgments">
<h2><span class="section-number">4.7. </span>Acknowledgments<a class="headerlink" href="#acknowledgments" title="Link to this heading">#</a></h2>
<p>Thanks to Google for creating the open-source course <a class="reference external" href="https://developers.google.com/machine-learning/testing-debugging">Testing and Debugging in Machine Learning</a> which is licensed under <a class="reference external" href="https://creativecommons.org/licenses/by/4.0/">CC-BY 4.0</a>. It contributes to the majority of <a class="reference internal" href="#model-evaluation"><span class="xref myst">Model evaluation</span></a>, <a class="reference internal" href="#model-deugging-improvement"><span class="xref myst">Model debugging &amp; improvement</span></a>, <a class="reference internal" href="#model-optimization"><span class="xref myst">Model optimization</span></a>, and assignments.</p>
<p>Thanks to <a class="reference external" href="https://github.com/chiphuyen">chiphuyen</a> for creating the <a class="reference external" href="https://huyenchip.com/machine-learning-systems-design/toc.html">Machine Learning Systems Design</a> which inspires some of the contents in this section.</p>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./book"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="data-engineering.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title"><span class="section-number">3. </span>Data engineering</p>
      </div>
    </a>
    <a class="right-next"
       href="model-deployment.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title"><span class="section-number">5. </span>Model deployment</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#model-selection">4.1. Model selection</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#start-simple">4.1.1. Start simple</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#transfer-learning">4.1.2. Transfer learning</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#automl">4.1.3. AutoML</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#model-evaluation">4.2. Model evaluation</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#evaluate-quality-using-model-metrics">4.2.1. Evaluate quality using model metrics</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#check-metrics-for-important-data-slices">4.2.2. Check metrics for important data slices</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#use-real-world-metrics">4.2.3. Use real-world metrics</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#optimizing-and-satisfying-metrics">4.2.4. Optimizing and satisfying metrics</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#model-debugging-improvement">4.3. Model debugging &amp; improvement</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#data-and-feature-debugging">4.3.1. Data and feature debugging</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#model-debugging">4.3.2. Model debugging</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#interpreting-loss-curves">4.3.3. Interpreting loss curves</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#model-optimization">4.4. Model optimization</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#add-useful-features">4.4.1. Add useful features</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#tune-hyperparameters">4.4.2. Tune hyperparameters</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#tune-model-depth-and-width">4.4.3. Tune model depth and width</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#your-turn">4.5. Your turn! 🚀</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#self-study">4.6. Self study</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#acknowledgments">4.7. Acknowledgments</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Xu Mingrui
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2024.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=8d27b9dea8ad943066ae"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=8d27b9dea8ad943066ae"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>